Question: Spark Shuffle Optimization Challenge

Scenario:
You are running a PySpark job that joins two large datasets (both are in Parquet format). The datasets are:
	1.	Customer Transactions (transactions_df) – 1 billion rows
	2.	Customer Profiles (customers_df) – 500 million rows

Each dataset is partitioned by country, but the Customer Profiles dataset is heavily skewed,
meaning some countries have significantly more data than others.



